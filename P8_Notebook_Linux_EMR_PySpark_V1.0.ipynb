{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8267953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"HADOOP_HOME\"] = \"C:/hadoop\"\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"  # Use your real JDK path!\n",
    "# os.environ[\"SPARK_HOME\"] = \"~/spark\"\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable      # chemin complet …\\python.exe\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Zulu\\\\zulu-11\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0dae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # Reduce it to fit in memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f526255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental.set_virtual_device_configuration(\n",
    "#     gpus[0],\n",
    "#     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])  # Limit to 4GB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d9256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = os.getcwd()\n",
    "# PATH_Data = PATH+'/data/Test1'\n",
    "# PATH_Result = PATH+'/data/Results'\n",
    "# print('PATH:        '+\\\n",
    "#       PATH+'\\nPATH_Data:   '+\\\n",
    "#       PATH_Data+'\\nPATH_Result: '+PATH_Result)\n",
    "\n",
    "PATH_Data = 'data/Test1'\n",
    "PATH_Result = 'data/Results'\n",
    "\n",
    "print('PATH:        '+\\\n",
    "      '\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba92550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bea157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"P8\")\n",
    "        .master(\"local[*]\")\n",
    "        # même interpréteur pour tout le monde\n",
    "        .config(\"spark.executorEnv.PYSPARK_PYTHON\",  sys.executable)\n",
    "        .config(\"spark.driver.python\",               sys.executable)\n",
    "        # facultatif mais utile :\n",
    "        .config(\"spark.python.worker.reuse\",         \"true\")     # évite de relancer N workers\n",
    "        .config(\"spark.python.worker.memory\",        \"2g\")       # limite mémoire par worker\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aeccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65640858",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"4\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8207725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc53ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd51ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "        arr = img_to_array(img)\n",
    "        return preprocess_input(arr)\n",
    "    except Exception as e:\n",
    "        print(\"Erreur image :\", e)\n",
    "        return np.zeros((224, 224, 3), dtype=np.float32)\n",
    "\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = images.repartition(2).select(\n",
    "    col(\"path\"), col(\"label\"),\n",
    "    featurize_udf(\"content\").alias(\"features\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20823e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "import numpy as np, os, sys, io\n",
    "from PIL import Image\n",
    "\n",
    "def just_size(content):\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(content))\n",
    "        return [float(img.size[0]), float(img.size[1])]\n",
    "    except Exception:\n",
    "        return [0.0, 0.0]\n",
    "\n",
    "dummy_udf = udf(just_size, ArrayType(FloatType()))\n",
    "\n",
    "test_df = images.limit(50).select(dummy_udf(\"content\").alias(\"wh\"))\n",
    "test_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [row.path for row in images.select(\"path\").collect()]\n",
    "\n",
    "def extract(path):\n",
    "    img = Image.open(path).resize((224,224))\n",
    "    arr = preprocess_input(img_to_array(img))\n",
    "    feat = backbone.predict(arr[None])[0].flatten()\n",
    "    return (path, feat.tolist())\n",
    "\n",
    "import multiprocessing as mp\n",
    "with mp.Pool() as pool:\n",
    "    rows = pool.map(extract, paths)\n",
    "\n",
    "# puis :\n",
    "spark.createDataFrame(rows, [\"path\",\"features\"]).write.parquet(PATH_Result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d07466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19243bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb933b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'exécution de cette cellule démarre l'application Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb788991",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad562eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 's3://p8-data'\n",
    "PATH_Data = PATH+'/Test'\n",
    "PATH_Result = PATH+'/Results'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bfeb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ab808",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9bc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d497f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8fe2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933100cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d760c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a930b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b387f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29205ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
