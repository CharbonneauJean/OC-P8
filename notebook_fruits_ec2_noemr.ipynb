{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3370e2fb",
   "metadata": {},
   "source": [
    "# Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Platform detection and configuration\n",
    "PLATFORM = platform.system()\n",
    "IS_EMR = 'EMR' in os.environ.get('AWS_EXECUTION_ENV', '')\n",
    "print(f\"Running on: {PLATFORM}\")\n",
    "print(f\"Is EMR: {IS_EMR}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set environment variables based on platform\n",
    "if PLATFORM == \"Windows\":\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "    if \"JAVA_HOME\" not in os.environ:\n",
    "        java_paths = [\"C:\\\\Program Files\\\\Java\\\\jdk-11\", \"C:\\\\Zulu\\\\zulu-11\"]\n",
    "        for path in java_paths:\n",
    "            if os.path.exists(path):\n",
    "                os.environ[\"JAVA_HOME\"] = path\n",
    "                break\n",
    "else:\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "    if \"JAVA_HOME\" not in os.environ:\n",
    "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, udf, element_at, split, count, when, lit\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, BooleanType, BinaryType\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, PCA\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259bc0a1",
   "metadata": {},
   "source": [
    "# Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    \"batch_size\": 64,  # Increased batch size\n",
    "    \"image_size\": (224, 224),\n",
    "    \"num_features\": 1280,\n",
    "    \"test_split\": 0.2,\n",
    "    \"seed\": 42,\n",
    "    # Random Forest parameters - optimized\n",
    "    \"num_trees\": 200,  # Increased from 100\n",
    "    \"max_depth\": 20,   # Increased from 15\n",
    "    \"min_instances_per_node\": 1,  # Reduced from 2\n",
    "    \"feature_subset_strategy\": \"sqrt\",\n",
    "    # PCA parameters - optimized\n",
    "    \"pca_components\": 100,  # Increased from 50\n",
    "    \"pca_variance_threshold\": 0.99  # Increased from 0.95\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Path configuration\n",
    "\n",
    "DATA_PATH = \"s3://oc-p8-charbonneau-fruits-input/Test1\"\n",
    "RESULTS_PATH = \"s3://oc-p8-charbonneau-fruits-output/Results\"\n",
    "MODEL_PATH = \"s3://oc-p8-charbonneau-fruits-input/models\"\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Results path: {RESULTS_PATH}\")\n",
    "print(f\"Model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d46dc7",
   "metadata": {},
   "source": [
    "# Disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU for TensorFlow to avoid memory issues in distributed processing\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Force TensorFlow to use CPU\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232cf8e",
   "metadata": {},
   "source": [
    "# Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "initialization_start = time.time()\n",
    "\n",
    "# Create Spark session with optimized configuration\n",
    "def create_spark_session():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"FruitClassification_TransferLearning_CrossPlatform\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "        .config(\"spark.python.worker.memory\", \"2g\") \\\n",
    "        .config(\"spark.python.worker.reuse\", \"true\")\n",
    "    \n",
    "    if not IS_EMR:\n",
    "        builder = builder.master(\"local[*]\") \\\n",
    "            .config(\"spark.driver.memory\", \"8g\") \\\n",
    "            .config(\"spark.executor.memory\", \"8g\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "            .config(\"spark.default.parallelism\", \"50\")\n",
    "    else:\n",
    "        builder = builder \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "    \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "spark = create_spark_session()\n",
    "sc = spark.sparkContext\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Number of executors: {sc._jsc.sc().getExecutorMemoryStatus().size()}\")\n",
    "print(f\"Default parallelism: {sc.defaultParallelism}\")\n",
    "initialization_time = time.time() - initialization_start\n",
    "print(f\"\\nSpark initialization time: {initialization_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b65edd",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model and Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f538e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Broadcast TensorFlow Model Weights\n",
    "print(\"=\"*80)\n",
    "print(\"TENSORFLOW MODEL BROADCASTING DEMONSTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "broadcast_start = time.time()\n",
    "\n",
    "# Load MobileNetV2 model\n",
    "print(\"\\n1. Loading MobileNetV2 pre-trained model...\")\n",
    "model_creation_start = time.time()\n",
    "\n",
    "# Create base model without top layers\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "model_creation_time = time.time() - model_creation_start\n",
    "print(f\"   Model creation time: {model_creation_time:.2f} seconds\")\n",
    "\n",
    "# Extract model weights and architecture\n",
    "print(\"\\n2. Extracting model weights and architecture...\")\n",
    "extraction_start = time.time()\n",
    "\n",
    "model_weights = base_model.get_weights()\n",
    "model_config = base_model.get_config()\n",
    "\n",
    "# Calculate model size\n",
    "weights_size = sum([w.nbytes for w in model_weights])\n",
    "print(f\"   Number of weight arrays: {len(model_weights)}\")\n",
    "print(f\"   Total weights size: {weights_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "extraction_time = time.time() - extraction_start\n",
    "print(f\"   Extraction time: {extraction_time:.2f} seconds\")\n",
    "\n",
    "# Serialize weights for broadcasting\n",
    "print(\"\\n3. Serializing weights for broadcast...\")\n",
    "serialization_start = time.time()\n",
    "\n",
    "serialized_weights = pickle.dumps(model_weights)\n",
    "serialized_size = len(serialized_weights)\n",
    "\n",
    "serialization_time = time.time() - serialization_start\n",
    "print(f\"   Serialized size: {serialized_size / (1024*1024):.2f} MB\")\n",
    "print(f\"   Serialization time: {serialization_time:.2f} seconds\")\n",
    "\n",
    "# Broadcast to all nodes\n",
    "print(\"\\n4. Broadcasting weights to all Spark nodes...\")\n",
    "broadcast_operation_start = time.time()\n",
    "\n",
    "# Clear any existing broadcasts\n",
    "if 'broadcast_weights' in locals():\n",
    "    try:\n",
    "        broadcast_weights.unpersist()\n",
    "    except:\n",
    "        pass\n",
    "if 'broadcast_config' in locals():\n",
    "    try:\n",
    "        broadcast_config.unpersist()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Broadcast the serialized weights\n",
    "broadcast_weights = sc.broadcast(serialized_weights)\n",
    "broadcast_config = sc.broadcast(model_config)\n",
    "\n",
    "broadcast_operation_time = time.time() - broadcast_operation_start\n",
    "print(f\"   Broadcast operation time: {broadcast_operation_time:.2f} seconds\")\n",
    "\n",
    "# Verify broadcast on executors\n",
    "print(\"\\n5. Verifying broadcast on executors...\")\n",
    "verification_start = time.time()\n",
    "\n",
    "def verify_broadcast_on_executor(partition_id):\n",
    "    \"\"\"Verify that broadcast variables are accessible on executor\"\"\"\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    # Force CPU usage\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    \n",
    "    try:\n",
    "        # Access broadcast variables\n",
    "        weights = pickle.loads(broadcast_weights.value)\n",
    "        config = broadcast_config.value\n",
    "        \n",
    "        # Recreate model to verify\n",
    "        with tf.device('/CPU:0'):\n",
    "            test_model = tf.keras.Model.from_config(config)\n",
    "            test_model.set_weights(weights)\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'partition_id': partition_id,\n",
    "            'weights_count': len(weights),\n",
    "            'config_layers': len(config.get('layers', [])),\n",
    "            'hostname': os.uname()[1] if hasattr(os, 'uname') else 'unknown'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'partition_id': partition_id,\n",
    "            'error': str(e),\n",
    "            'hostname': os.uname()[1] if hasattr(os, 'uname') else 'unknown'\n",
    "        }\n",
    "\n",
    "# Test on multiple partitions to verify distribution\n",
    "num_partitions = min(sc.defaultParallelism, 4)\n",
    "verification_rdd = sc.parallelize(range(num_partitions), num_partitions)\n",
    "verification_results = verification_rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, iterator: [verify_broadcast_on_executor(idx)]\n",
    ").collect()\n",
    "\n",
    "verification_time = time.time() - verification_start\n",
    "print(f\"   Verification time: {verification_time:.2f} seconds\")\n",
    "\n",
    "# Display verification results\n",
    "print(\"\\n6. Broadcast Verification Results:\")\n",
    "success_count = 0\n",
    "for result in verification_results:\n",
    "    if result['success']:\n",
    "        success_count += 1\n",
    "        print(f\"   Partition {result['partition_id']}: ✓ Success - \"\n",
    "              f\"{result['weights_count']} weight arrays, \"\n",
    "              f\"{result['config_layers']} layers on {result['hostname']}\")\n",
    "    else:\n",
    "        print(f\"   Partition {result['partition_id']}: ✗ Failed - {result['error']}\")\n",
    "\n",
    "# Calculate total broadcast time\n",
    "total_broadcast_time = time.time() - broadcast_start\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BROADCAST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model type: MobileNetV2 (Transfer Learning)\")\n",
    "print(f\"Original model size: {weights_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Broadcast size: {serialized_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Compression ratio: {weights_size / serialized_size:.2f}x\")\n",
    "print(f\"Number of executors: {sc._jsc.sc().getExecutorMemoryStatus().size()}\")\n",
    "print(f\"Broadcast verified on {success_count}/{len(verification_results)} partitions\")\n",
    "print(f\"\\nTiming breakdown:\")\n",
    "print(f\"  - Model creation: {model_creation_time:.2f}s\")\n",
    "print(f\"  - Weight extraction: {extraction_time:.2f}s\")\n",
    "print(f\"  - Serialization: {serialization_time:.2f}s\")\n",
    "print(f\"  - Broadcast operation: {broadcast_operation_time:.2f}s\")\n",
    "print(f\"  - Verification: {verification_time:.2f}s\")\n",
    "print(f\"  - Total time: {total_broadcast_time:.2f}s\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store timing for final summary\n",
    "model_load_time = total_broadcast_time\n",
    "\n",
    "# Additional demonstration: Show that broadcast is accessible in transformations\n",
    "print(\"\\n7. Testing broadcast in a transformation...\")\n",
    "test_start = time.time()\n",
    "\n",
    "def test_broadcast_in_transformation(x):\n",
    "    \"\"\"Test accessing broadcast in a transformation\"\"\"\n",
    "    try:\n",
    "        weights = pickle.loads(broadcast_weights.value)\n",
    "        config = broadcast_config.value\n",
    "        return f\"Success: {len(weights)} weights, {len(config.get('layers', []))} layers\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test on a small RDD\n",
    "test_rdd = sc.parallelize([1, 2, 3], 3)\n",
    "test_results = test_rdd.map(test_broadcast_in_transformation).collect()\n",
    "\n",
    "test_time = time.time() - test_start\n",
    "print(f\"   Transformation test time: {test_time:.2f} seconds\")\n",
    "print(\"   Results from each partition:\")\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"     Partition {i}: {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Broadcast successfully distributed and verified!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f8c93",
   "metadata": {},
   "source": [
    "# Load and Prepare Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "loading_start = time.time()\n",
    "print(f\"Loading images from: {DATA_PATH}\")\n",
    "\n",
    "# Read images with labels from folder structure\n",
    "image_read_start = time.time()\n",
    "images_df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(DATA_PATH)\n",
    "image_read_time = time.time() - image_read_start\n",
    "print(f\"Image reading time: {image_read_time:.2f} seconds\")\n",
    "\n",
    "# Extract labels from folder names\n",
    "label_extraction_start = time.time()\n",
    "images_df = images_df.withColumn('label', element_at(split(col('path'), '/'), -2))\n",
    "label_extraction_time = time.time() - label_extraction_start\n",
    "print(f\"Label extraction time: {label_extraction_time:.2f} seconds\")\n",
    "\n",
    "# Add image validation\n",
    "validation_start = time.time()\n",
    "def validate_image(content):\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(content))\n",
    "        if img.size[0] > 0 and img.size[1] > 0:\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "validate_image_udf = udf(validate_image, BooleanType())\n",
    "images_df = images_df.withColumn('is_valid', validate_image_udf(col('content')))\n",
    "\n",
    "# Filter out corrupted images\n",
    "images_df = images_df.filter(col('is_valid') == True).drop('is_valid')\n",
    "validation_time = time.time() - validation_start\n",
    "print(f\"Image validation time: {validation_time:.2f} seconds\")\n",
    "\n",
    "# Cache the dataframe\n",
    "cache_start = time.time()\n",
    "images_df.cache()\n",
    "cache_time = time.time() - cache_start\n",
    "print(f\"Caching time: {cache_time:.2f} seconds\")\n",
    "\n",
    "# Count images per class\n",
    "count_start = time.time()\n",
    "image_count = images_df.count()\n",
    "class_distribution = images_df.groupBy('label').count().orderBy('label').collect()\n",
    "count_time = time.time() - count_start\n",
    "print(f\"Counting time: {count_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nTotal valid images loaded: {image_count}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "for row in class_distribution:\n",
    "    print(f\"  {row['label']}: {row['count']} images\")\n",
    "\n",
    "loading_time = time.time() - loading_start\n",
    "print(f\"\\nTotal data loading time: {loading_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7430d55",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation for training data\n",
    "def augment_image(image_bytes):\n",
    "    \"\"\"Apply data augmentation to improve model generalization\"\"\"\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "        # Random rotation\n",
    "        if np.random.random() > 0.5:\n",
    "            angle = np.random.uniform(-20, 20)\n",
    "            img = img.rotate(angle, fillcolor=(255, 255, 255))\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        if np.random.random() > 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        # Random brightness adjustment\n",
    "        if np.random.random() > 0.5:\n",
    "            from PIL import ImageEnhance\n",
    "            enhancer = ImageEnhance.Brightness(img)\n",
    "            img = enhancer.enhance(np.random.uniform(0.8, 1.2))\n",
    "        \n",
    "        # Convert back to bytes\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        img.save(img_byte_arr, format='JPEG')\n",
    "        return img_byte_arr.getvalue()\n",
    "    except:\n",
    "        return image_bytes\n",
    "\n",
    "# Apply augmentation to training data only (do this after train/test split)\n",
    "augment_udf = udf(augment_image, BinaryType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06aaafe",
   "metadata": {},
   "source": [
    "# Feature Extraction with Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69afb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with Transfer Learning\n",
    "from pyspark import StorageLevel  # Add this import\n",
    "\n",
    "# Start timing for feature extraction\n",
    "feature_extraction_start = time.time()\n",
    "\n",
    "def extract_mobilenet_features_batch(image_bytes_list):\n",
    "    \"\"\"Extract features for a batch of images using MobileNetV2\"\"\"\n",
    "    try:\n",
    "        # Recreate model from broadcast weights - with CPU only\n",
    "        with tf.device('/CPU:0'):\n",
    "            weights = pickle.loads(broadcast_weights.value)\n",
    "            config = broadcast_config.value\n",
    "            \n",
    "            # Recreate the model\n",
    "            base_model = tf.keras.Model.from_config(config)\n",
    "            base_model.set_weights(weights)\n",
    "            \n",
    "            # Process batch\n",
    "            batch_features = []\n",
    "            for image_bytes in image_bytes_list:\n",
    "                try:\n",
    "                    # Load and preprocess image\n",
    "                    img = Image.open(io.BytesIO(image_bytes))\n",
    "                    img = img.convert('RGB')\n",
    "                    img = img.resize((224, 224))\n",
    "                    \n",
    "                    # Convert to array and preprocess\n",
    "                    img_array = image.img_to_array(img)\n",
    "                    img_array = np.expand_dims(img_array, axis=0)\n",
    "                    img_array = preprocess_input(img_array)\n",
    "                    \n",
    "                    # Extract features\n",
    "                    features = base_model.predict(img_array, verbose=0)\n",
    "                    batch_features.append(features[0].tolist())\n",
    "                except Exception as e:\n",
    "                    # Return zeros if error for this image\n",
    "                    batch_features.append([0.0] * CONFIG[\"num_features\"])\n",
    "            \n",
    "            return batch_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return zeros for entire batch if error\n",
    "        return [[0.0] * CONFIG[\"num_features\"]] * len(image_bytes_list)\n",
    "\n",
    "# Process images in smaller batches to avoid memory issues\n",
    "def process_partition(iterator):\n",
    "    \"\"\"Process a partition of images\"\"\"\n",
    "    # Convert iterator to list\n",
    "    rows = list(iterator)\n",
    "    if not rows:\n",
    "        return iter([])\n",
    "    \n",
    "    # Process in smaller batches\n",
    "    batch_size = 32  # Reduced batch size\n",
    "    result_rows = []\n",
    "    \n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        batch_rows = rows[i:i+batch_size]\n",
    "        image_bytes_list = [row.content for row in batch_rows]\n",
    "        \n",
    "        # Extract features for the batch\n",
    "        features_list = extract_mobilenet_features_batch(image_bytes_list)\n",
    "        \n",
    "        # Create result rows\n",
    "        for j, row in enumerate(batch_rows):\n",
    "            result_rows.append(Row(\n",
    "                path=row.path,\n",
    "                label=row.label,\n",
    "                features=features_list[j]\n",
    "            ))\n",
    "    \n",
    "    return iter(result_rows)\n",
    "\n",
    "# Apply batch processing with reduced partitions\n",
    "optimal_partitions = min(sc.defaultParallelism, 20)  # Reduced from 50\n",
    "images_df_repartitioned = images_df.repartition(optimal_partitions)\n",
    "features_rdd = images_df_repartitioned.rdd.mapPartitions(process_partition)\n",
    "features_df = spark.createDataFrame(features_rdd)\n",
    "\n",
    "# Convert to vector format\n",
    "array_to_vector_udf = udf(lambda x: Vectors.dense(x) if x else Vectors.dense([0.0] * CONFIG[\"num_features\"]), VectorUDT())\n",
    "features_vector_df = features_df.select(\n",
    "    col(\"path\"),\n",
    "    col(\"label\"),\n",
    "    array_to_vector_udf(col(\"features\")).alias(\"features_vector\")\n",
    ")\n",
    "\n",
    "# Cache with disk spillover to avoid memory issues\n",
    "features_vector_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Calculate feature extraction time correctly\n",
    "feature_extraction_time = time.time() - feature_extraction_start\n",
    "print(f\"\\nFeature extraction time: {feature_extraction_time:.2f} seconds\")\n",
    "\n",
    "# Force computation and show count\n",
    "processed_count = features_vector_df.count()\n",
    "print(f\"Successfully extracted features for {processed_count} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887efc72",
   "metadata": {},
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e412932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Analysis\n",
    "pca_start = time.time()\n",
    "print(\"Performing PCA analysis...\")\n",
    "\n",
    "# Prepare data with label indexing\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "label_indexer_model = label_indexer.fit(features_vector_df)\n",
    "indexed_df = label_indexer_model.transform(features_vector_df)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vector\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "scaler_model = scaler.fit(indexed_df)\n",
    "scaled_df = scaler_model.transform(indexed_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(k=CONFIG[\"pca_components\"], inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(scaled_df)\n",
    "pca_df = pca_model.transform(scaled_df)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = pca_model.explainedVariance.toArray()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"Components: {CONFIG['pca_components']}\")\n",
    "print(f\"Total variance explained: {cumulative_variance[-1]:.2%}\")\n",
    "\n",
    "# Find number of components for threshold\n",
    "n_components_threshold = np.argmax(cumulative_variance >= CONFIG[\"pca_variance_threshold\"]) + 1\n",
    "print(f\"Components needed for {CONFIG['pca_variance_threshold']:.0%} variance: {n_components_threshold}\")\n",
    "\n",
    "pca_time = time.time() - pca_start\n",
    "print(f\"\\nPCA analysis time: {pca_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f4542",
   "metadata": {},
   "source": [
    "# Visualize PCA Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create 2D PCA for visualization\n",
    "pca_2d = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_2d\")\n",
    "pca_2d_model = pca_2d.fit(scaled_df)\n",
    "pca_2d_result = pca_2d_model.transform(scaled_df)\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "pca_viz_df = pca_2d_result.select(\"label\", \"pca_2d\").toPandas()\n",
    "pca_viz_df['pca_1'] = pca_viz_df['pca_2d'].apply(lambda x: float(x[0]))\n",
    "pca_viz_df['pca_2'] = pca_viz_df['pca_2d'].apply(lambda x: float(x[1]))\n",
    "\n",
    "# Plot PCA visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=pca_viz_df, x='pca_1', y='pca_2', hue='label', \n",
    "                palette='viridis', alpha=0.6, s=50)\n",
    "plt.title('PCA Visualization of MobileNetV2 Features', fontsize=16)\n",
    "plt.xlabel(f'First Principal Component')\n",
    "plt.ylabel(f'Second Principal Component')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.xticks(range(1, len(explained_variance) + 1, 5))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "plt.axhline(y=CONFIG[\"pca_variance_threshold\"], color='r', linestyle='--', \n",
    "            label=f'{CONFIG[\"pca_variance_threshold\"]:.0%} threshold')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.xticks(range(0, len(cumulative_variance) + 1, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ce6ff",
   "metadata": {},
   "source": [
    "# Check similar fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and visualize similar fruits in PCA space\n",
    "from pyspark.sql.functions import sqrt, pow as spark_pow\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "print(\"Finding similar fruits in PCA space...\")\n",
    "\n",
    "# First, let's get the PCA features with paths\n",
    "# Use pca_df instead of pca_result\n",
    "pca_with_paths = pca_df.select(\"path\", \"label\", \"pca_features\", \"label_index\")\n",
    "\n",
    "# Convert PCA features to individual columns for distance calculation\n",
    "def extract_pca_components(vector):\n",
    "    return float(vector[0]), float(vector[1])\n",
    "\n",
    "extract_pca_udf = udf(extract_pca_components, ArrayType(FloatType()))\n",
    "\n",
    "# Add PCA components as separate columns\n",
    "pca_with_components = pca_with_paths.select(\n",
    "    col(\"path\"),\n",
    "    col(\"label\"),\n",
    "    col(\"label_index\"),\n",
    "    col(\"pca_features\"),\n",
    "    element_at(extract_pca_udf(col(\"pca_features\")), 1).alias(\"pca_1\"),\n",
    "    element_at(extract_pca_udf(col(\"pca_features\")), 2).alias(\"pca_2\")\n",
    ")\n",
    "\n",
    "# Cache for performance\n",
    "pca_with_components.cache()\n",
    "\n",
    "# Function to find nearest neighbors in PCA space\n",
    "def find_similar_fruits(df, num_examples=5, num_neighbors=5):\n",
    "    \"\"\"Find examples of different fruits that are close in PCA space\"\"\"\n",
    "    \n",
    "    # Get a sample point from each class\n",
    "    sample_points = []\n",
    "    labels_list = df.select(\"label\").distinct().collect()\n",
    "    \n",
    "    for label_row in labels_list[:num_examples]:  # Limit to num_examples classes\n",
    "        label = label_row[\"label\"]\n",
    "        # Get one sample from this class\n",
    "        sample = df.filter(col(\"label\") == label).limit(1).collect()[0]\n",
    "        if sample:\n",
    "            sample_points.append({\n",
    "                \"label\": label,\n",
    "                \"path\": sample[\"path\"],\n",
    "                \"pca_1\": sample[\"pca_1\"],\n",
    "                \"pca_2\": sample[\"pca_2\"]\n",
    "            })\n",
    "    \n",
    "    # For each sample point, find nearest neighbors from different classes\n",
    "    similar_pairs = []\n",
    "    \n",
    "    for sample in sample_points:\n",
    "        # Calculate distances to all other points\n",
    "        distances_df = df.filter(col(\"label\") != sample[\"label\"]).select(\n",
    "            col(\"path\"),\n",
    "            col(\"label\"),\n",
    "            col(\"pca_1\"),\n",
    "            col(\"pca_2\"),\n",
    "            sqrt(\n",
    "                spark_pow(col(\"pca_1\") - lit(sample[\"pca_1\"]), 2) + \n",
    "                spark_pow(col(\"pca_2\") - lit(sample[\"pca_2\"]), 2)\n",
    "            ).alias(\"distance\")\n",
    "        )\n",
    "        \n",
    "        # Get top N nearest neighbors\n",
    "        nearest = distances_df.orderBy(\"distance\").limit(num_neighbors).collect()\n",
    "        \n",
    "        similar_pairs.append({\n",
    "            \"source\": sample,\n",
    "            \"neighbors\": nearest\n",
    "        })\n",
    "    \n",
    "    return similar_pairs\n",
    "\n",
    "# Find similar fruits\n",
    "similar_fruits = find_similar_fruits(pca_with_components, num_examples=5, num_neighbors=3)\n",
    "\n",
    "# Visualize the similar fruits\n",
    "fig, axes = plt.subplots(len(similar_fruits), 4, figsize=(16, 4*len(similar_fruits)))\n",
    "if len(similar_fruits) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, pair in enumerate(similar_fruits):\n",
    "    source = pair[\"source\"]\n",
    "    neighbors = pair[\"neighbors\"]\n",
    "    \n",
    "    # Load and display source image\n",
    "    try:\n",
    "        # Read the source image\n",
    "        source_img_df = images_df.filter(col(\"path\") == source[\"path\"]).select(\"content\").collect()\n",
    "        if source_img_df:\n",
    "            source_img = Image.open(io.BytesIO(source_img_df[0][\"content\"]))\n",
    "            axes[idx, 0].imshow(source_img)\n",
    "            axes[idx, 0].set_title(f\"Source: {source['label']}\\nPCA: ({source['pca_1']:.2f}, {source['pca_2']:.2f})\", \n",
    "                                  fontsize=10)\n",
    "            axes[idx, 0].axis('off')\n",
    "    except:\n",
    "        axes[idx, 0].text(0.5, 0.5, f\"Source: {source['label']}\", ha='center', va='center')\n",
    "        axes[idx, 0].axis('off')\n",
    "    \n",
    "    # Display nearest neighbors\n",
    "    for j, neighbor in enumerate(neighbors[:3]):  # Show top 3 neighbors\n",
    "        try:\n",
    "            # Read the neighbor image\n",
    "            neighbor_img_df = images_df.filter(col(\"path\") == neighbor[\"path\"]).select(\"content\").collect()\n",
    "            if neighbor_img_df:\n",
    "                neighbor_img = Image.open(io.BytesIO(neighbor_img_df[0][\"content\"]))\n",
    "                axes[idx, j+1].imshow(neighbor_img)\n",
    "                axes[idx, j+1].set_title(f\"{neighbor['label']}\\nDist: {neighbor['distance']:.3f}\", \n",
    "                                        fontsize=10)\n",
    "                axes[idx, j+1].axis('off')\n",
    "        except:\n",
    "            axes[idx, j+1].text(0.5, 0.5, f\"{neighbor['label']}\", ha='center', va='center')\n",
    "            axes[idx, j+1].axis('off')\n",
    "\n",
    "plt.suptitle('Similar Fruits in PCA Space\\n(Source fruit on left, nearest neighbors from different classes on right)', \n",
    "             fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot highlighting the similar pairs\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot all points\n",
    "pca_viz_df = pca_with_components.select(\"label\", \"pca_1\", \"pca_2\").toPandas()\n",
    "sns.scatterplot(data=pca_viz_df, x='pca_1', y='pca_2', hue='label', \n",
    "                palette='viridis', alpha=0.3, s=30, legend=False)\n",
    "\n",
    "# Highlight the similar pairs\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for idx, pair in enumerate(similar_fruits[:5]):\n",
    "    source = pair[\"source\"]\n",
    "    neighbors = pair[\"neighbors\"]\n",
    "    \n",
    "    # Plot source point\n",
    "    plt.scatter(source['pca_1'], source['pca_2'], \n",
    "               color=colors[idx % len(colors)], s=200, marker='*', \n",
    "               edgecolors='black', linewidth=2,\n",
    "               label=f\"Source: {source['label']}\")\n",
    "    \n",
    "    # Plot connections to neighbors\n",
    "    for neighbor in neighbors[:3]:\n",
    "        plt.plot([source['pca_1'], neighbor['pca_1']], \n",
    "                [source['pca_2'], neighbor['pca_2']], \n",
    "                color=colors[idx % len(colors)], alpha=0.5, linestyle='--')\n",
    "        plt.scatter(neighbor['pca_1'], neighbor['pca_2'], \n",
    "                   color=colors[idx % len(colors)], s=100, marker='o',\n",
    "                   edgecolors='black', linewidth=1)\n",
    "\n",
    "plt.title('PCA Space: Similar Fruits from Different Classes', fontsize=16)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary of findings\n",
    "print(\"\\nSummary of Similar Fruits in PCA Space:\")\n",
    "print(\"=\"*60)\n",
    "for idx, pair in enumerate(similar_fruits):\n",
    "    source = pair[\"source\"]\n",
    "    print(f\"\\nSource fruit: {source['label']}\")\n",
    "    print(f\"PCA coordinates: ({source['pca_1']:.3f}, {source['pca_2']:.3f})\")\n",
    "    print(\"Nearest neighbors from different classes:\")\n",
    "    for j, neighbor in enumerate(pair[\"neighbors\"][:3]):\n",
    "        print(f\"  {j+1}. {neighbor['label']} - Distance: {neighbor['distance']:.3f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20734c4",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7576f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Random Forest Training without Cross-Validation\n",
    "training_start = time.time()\n",
    "\n",
    "# Prepare train/test split\n",
    "train_df, test_df = pca_df.randomSplit([1-CONFIG[\"test_split\"], CONFIG[\"test_split\"]], \n",
    "                                       seed=CONFIG[\"seed\"])\n",
    "\n",
    "# Count samples\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "print(f\"Training samples: {train_count}\")\n",
    "print(f\"Test samples: {test_count}\")\n",
    "\n",
    "# Define Random Forest with optimized parameters for fruit classification\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"pca_features\",\n",
    "    labelCol=\"label_index\",\n",
    "    numTrees=150,  # Good balance between performance and speed\n",
    "    maxDepth=30,   # Prevents overfitting while maintaining accuracy\n",
    "    minInstancesPerNode=2,  # Helps with generalization\n",
    "    featureSubsetStrategy=\"sqrt\",  # Standard for classification\n",
    "    seed=CONFIG[\"seed\"],\n",
    "    maxBins=32,  # Default value, good for most cases\n",
    "    subsamplingRate=0.8  # Helps prevent overfitting\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "model_fit_start = time.time()\n",
    "rf_model = rf.fit(train_df)\n",
    "model_fit_time = time.time() - model_fit_start\n",
    "\n",
    "print(f\"Model training completed in {model_fit_time:.2f} seconds\")\n",
    "training_time = time.time() - training_start\n",
    "print(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nRandom Forest Model Summary:\")\n",
    "print(f\"- Number of trees: {rf.getNumTrees()}\")\n",
    "print(f\"- Max depth: {rf.getMaxDepth()}\")\n",
    "print(f\"- Feature subset strategy: {rf.getFeatureSubsetStrategy()}\")\n",
    "print(f\"- Number of features: {CONFIG['pca_components']}\")\n",
    "print(f\"- Number of classes: {len(label_indexer_model.labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fedc7b",
   "metadata": {},
   "source": [
    "# Class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle class imbalance\n",
    "from pyspark.sql.functions import col, count, min as spark_min\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = train_df.groupBy(\"label_index\").agg(count(\"*\").alias(\"count\"))\n",
    "min_count = class_counts.agg(spark_min(\"count\")).collect()[0][0]\n",
    "\n",
    "# Create balanced dataset by oversampling minority classes\n",
    "balanced_dfs = []\n",
    "for row in class_counts.collect():\n",
    "    label = row[\"label_index\"]\n",
    "    current_count = row[\"count\"]\n",
    "    oversample_ratio = min_count / current_count\n",
    "    \n",
    "    class_df = train_df.filter(col(\"label_index\") == label)\n",
    "    if oversample_ratio < 1:\n",
    "        # Oversample minority class\n",
    "        oversampled_df = class_df.sample(withReplacement=True, fraction=1.0/oversample_ratio, seed=CONFIG[\"seed\"])\n",
    "        balanced_dfs.append(oversampled_df)\n",
    "    else:\n",
    "        balanced_dfs.append(class_df)\n",
    "\n",
    "# Combine balanced datasets\n",
    "from functools import reduce\n",
    "balanced_train_df = reduce(lambda df1, df2: df1.union(df2), balanced_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02dae5c",
   "metadata": {},
   "source": [
    "# Train Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "preparation_start = time.time()\n",
    "\n",
    "# Check if train_df already exists from cross-validation\n",
    "if 'train_df' not in locals():\n",
    "    split_start = time.time()\n",
    "    # Use PCA features for training\n",
    "    train_df, test_df = pca_df.randomSplit([1-CONFIG[\"test_split\"], CONFIG[\"test_split\"]], \n",
    "                                           seed=CONFIG[\"seed\"])\n",
    "    split_time = time.time() - split_start\n",
    "    print(f\"Train/test split time: {split_time:.2f} seconds\")\n",
    "\n",
    "count_start = time.time()\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "count_time = time.time() - count_start\n",
    "print(f\"Sample counting time: {count_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\nTraining samples: {train_count}\")\n",
    "print(f\"Test samples: {test_count}\")\n",
    "\n",
    "# Train Random Forest on PCA features (skip if using cross-validation model)\n",
    "if 'rf_model' not in locals():\n",
    "    training_start = time.time()\n",
    "    print(\"\\nTraining Random Forest model on PCA features...\")\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"pca_features\",\n",
    "        labelCol=\"label_index\",\n",
    "        numTrees=CONFIG[\"num_trees\"],\n",
    "        maxDepth=CONFIG[\"max_depth\"],\n",
    "        minInstancesPerNode=CONFIG[\"min_instances_per_node\"],\n",
    "        featureSubsetStrategy=CONFIG[\"feature_subset_strategy\"],\n",
    "        seed=CONFIG[\"seed\"]\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model_fit_start = time.time()\n",
    "    rf_model = rf.fit(train_df)\n",
    "    model_fit_time = time.time() - model_fit_start\n",
    "    print(f\"Model fitting time: {model_fit_time:.2f} seconds\")\n",
    "    \n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"Total model training time: {training_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"\\nUsing cross-validated model with best parameters\")\n",
    "    training_time = 0  # Already measured in cross-validation\n",
    "\n",
    "preparation_time = time.time() - preparation_start\n",
    "print(f\"Total preparation time: {preparation_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cff898",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importances = rf_model.featureImportances.toArray()\n",
    "top_features_idx = np.argsort(feature_importances)[-20:][::-1]\n",
    "\n",
    "print(\"\\nTop 20 Most Important PCA Components:\")\n",
    "for idx in top_features_idx:\n",
    "    print(f\"PCA Component {idx}: {feature_importances[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed3a4d3",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluation_start = time.time()\n",
    "print(\"Evaluating model performance...\")\n",
    "\n",
    "# Make predictions\n",
    "prediction_start = time.time()\n",
    "predictions = rf_model.transform(test_df)\n",
    "prediction_time = time.time() - prediction_start\n",
    "print(f\"Prediction time: {prediction_time:.2f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_start = time.time()\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "metrics_time = time.time() - metrics_start\n",
    "print(f\"Metrics calculation time: {metrics_time:.2f} seconds\")\n",
    "\n",
    "evaluation_time = time.time() - evaluation_start\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODEL PERFORMANCE RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"F1 Score:  {f1_score:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"\\nTotal evaluation time: {evaluation_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb97be",
   "metadata": {},
   "source": [
    "# Test with ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble approach with multiple classifiers\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "\n",
    "# Create multiple models\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"pca_features\",\n",
    "    labelCol=\"label_index\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"pca_features\",\n",
    "    labelCol=\"label_index\",\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=1,\n",
    "    seed=CONFIG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Train all models\n",
    "print(\"Training ensemble models...\")\n",
    "lr_model = lr.fit(train_df)\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# Make predictions with all models\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "\n",
    "# Ensemble voting (majority vote)\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "ensemble_predictions = rf_predictions.select(\"path\", \"label_index\", \n",
    "                                           col(\"prediction\").alias(\"rf_pred\")) \\\n",
    "    .join(lr_predictions.select(\"path\", col(\"prediction\").alias(\"lr_pred\")), \"path\") \\\n",
    "    .join(dt_predictions.select(\"path\", col(\"prediction\").alias(\"dt_pred\")), \"path\")\n",
    "\n",
    "# Majority voting\n",
    "ensemble_predictions = ensemble_predictions.withColumn(\n",
    "    \"prediction\",\n",
    "    when((col(\"rf_pred\") == col(\"lr_pred\")) | (col(\"rf_pred\") == col(\"dt_pred\")), col(\"rf_pred\"))\n",
    "    .when(col(\"lr_pred\") == col(\"dt_pred\"), col(\"lr_pred\"))\n",
    "    .otherwise(col(\"rf_pred\"))  # Default to RF if no majority\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_accuracy = evaluator_accuracy.evaluate(ensemble_predictions)\n",
    "print(f\"\\nEnsemble Model Accuracy: {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd98185",
   "metadata": {},
   "source": [
    "# Save Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ebffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total time\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Helper function to convert numpy types to Python native types\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Prepare results with detailed timing breakdown\n",
    "results = {\n",
    "    \"platform\": {\n",
    "        \"system\": PLATFORM,\n",
    "        \"is_emr\": IS_EMR,\n",
    "        \"spark_version\": spark.version,\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"executors\": int(sc._jsc.sc().getExecutorMemoryStatus().size()),\n",
    "        \"default_parallelism\": int(sc.defaultParallelism)\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"total_images\": int(image_count),\n",
    "        \"processed_images\": int(processed_count),\n",
    "        \"train_samples\": int(train_count),\n",
    "        \"test_samples\": int(test_count),\n",
    "        \"num_classes\": len(label_indexer_model.labels),\n",
    "        \"classes\": label_indexer_model.labels\n",
    "    },\n",
    "    \"feature_extraction\": {\n",
    "        \"method\": \"MobileNetV2 Transfer Learning\",\n",
    "        \"input_size\": list(CONFIG[\"image_size\"]),\n",
    "        \"feature_dimension\": int(CONFIG[\"num_features\"]),\n",
    "        \"weights_broadcast_size_mb\": float(len(serialized_weights) / (1024*1024))\n",
    "    },\n",
    "    \"pca_analysis\": {\n",
    "        \"components_used\": int(CONFIG[\"pca_components\"]),\n",
    "        \"variance_explained\": float(cumulative_variance[-1]),\n",
    "        \"components_for_threshold\": int(n_components_threshold)\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"RandomForestClassifier\",\n",
    "        \"num_trees\": int(CONFIG[\"num_trees\"]),\n",
    "        \"max_depth\": int(CONFIG[\"max_depth\"]),\n",
    "        \"input_features\": int(CONFIG[\"pca_components\"])\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"f1_score\": float(f1_score),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"ensemble_accuracy\": float(ensemble_accuracy) if 'ensemble_accuracy' in locals() else None\n",
    "    },\n",
    "    \"timing\": {\n",
    "        \"initialization\": {\n",
    "            \"spark_init\": float(initialization_time),\n",
    "            \"model_load_broadcast\": float(model_load_time),\n",
    "            \"total\": float(initialization_time + model_load_time)\n",
    "        },\n",
    "        \"data_loading\": {\n",
    "            \"image_reading\": float(image_read_time) if 'image_read_time' in locals() else None,\n",
    "            \"label_extraction\": float(label_extraction_time) if 'label_extraction_time' in locals() else None,\n",
    "            \"validation\": float(validation_time) if 'validation_time' in locals() else None,\n",
    "            \"caching\": float(cache_time) if 'cache_time' in locals() else None,\n",
    "            \"counting\": float(count_time) if 'count_time' in locals() else None,\n",
    "            \"total\": float(loading_time)\n",
    "        },\n",
    "        \"feature_extraction\": {\n",
    "            \"total\": float(feature_extraction_time)\n",
    "        },\n",
    "        \"pca_analysis\": {\n",
    "            \"total\": float(pca_time)\n",
    "        },\n",
    "        \"model_training\": {\n",
    "            \"preparation\": float(preparation_time) if 'preparation_time' in locals() else None,\n",
    "            \"training\": float(training_time),\n",
    "            \"total\": float(training_time + (preparation_time if 'preparation_time' in locals() else 0))\n",
    "        },\n",
    "        \"evaluation\": {\n",
    "            \"prediction\": float(prediction_time) if 'prediction_time' in locals() else None,\n",
    "            \"metrics\": float(metrics_time) if 'metrics_time' in locals() else None,\n",
    "            \"total\": float(evaluation_time)\n",
    "        },\n",
    "        \"total_execution_time\": float(total_time)\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Convert all values to be JSON serializable\n",
    "results = convert_to_serializable(results)\n",
    "\n",
    "# Print detailed timing summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DETAILED TIMING BREAKDOWN\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n1. Initialization Phase:\")\n",
    "print(f\"   - Spark initialization: {results['timing']['initialization']['spark_init']:.2f}s\")\n",
    "print(f\"   - Model load & broadcast: {results['timing']['initialization']['model_load_broadcast']:.2f}s\")\n",
    "print(f\"   - Subtotal: {results['timing']['initialization']['total']:.2f}s\")\n",
    "\n",
    "print(f\"\\n2. Data Loading Phase:\")\n",
    "if results['timing']['data_loading']['image_reading']:\n",
    "    print(f\"   - Image reading: {results['timing']['data_loading']['image_reading']:.2f}s\")\n",
    "    print(f\"   - Label extraction: {results['timing']['data_loading']['label_extraction']:.2f}s\")\n",
    "    print(f\"   - Validation: {results['timing']['data_loading']['validation']:.2f}s\")\n",
    "    print(f\"   - Caching: {results['timing']['data_loading']['caching']:.2f}s\")\n",
    "    print(f\"   - Counting: {results['timing']['data_loading']['counting']:.2f}s\")\n",
    "print(f\"   - Subtotal: {results['timing']['data_loading']['total']:.2f}s\")\n",
    "\n",
    "print(f\"\\n3. Feature Extraction Phase:\")\n",
    "print(f\"   - Total: {results['timing']['feature_extraction']['total']:.2f}s\")\n",
    "print(f\"   - Per image: {results['timing']['feature_extraction']['total']/results['data']['processed_images']*1000:.2f}ms\")\n",
    "\n",
    "print(f\"\\n4. PCA Analysis Phase:\")\n",
    "print(f\"   - Total: {results['timing']['pca_analysis']['total']:.2f}s\")\n",
    "\n",
    "print(f\"\\n5. Model Training Phase:\")\n",
    "if results['timing']['model_training']['preparation']:\n",
    "    print(f\"   - Preparation: {results['timing']['model_training']['preparation']:.2f}s\")\n",
    "print(f\"   - Training: {results['timing']['model_training']['training']:.2f}s\")\n",
    "print(f\"   - Subtotal: {results['timing']['model_training']['total']:.2f}s\")\n",
    "\n",
    "print(f\"\\n6. Evaluation Phase:\")\n",
    "if results['timing']['evaluation']['prediction']:\n",
    "    print(f\"   - Prediction: {results['timing']['evaluation']['prediction']:.2f}s\")\n",
    "    print(f\"   - Metrics calculation: {results['timing']['evaluation']['metrics']:.2f}s\")\n",
    "print(f\"   - Subtotal: {results['timing']['evaluation']['total']:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTAL EXECUTION TIME: {results['timing']['total_execution_time']:.2f}s ({results['timing']['total_execution_time']/60:.2f} minutes)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Platform: {results['platform']['system']} {'(EMR)' if IS_EMR else '(Local)'}\")\n",
    "print(f\"Feature Extraction: {results['feature_extraction']['method']}\")\n",
    "print(f\"Model Broadcast Size: {results['feature_extraction']['weights_broadcast_size_mb']:.2f} MB\")\n",
    "print(f\"\\nPCA Analysis:\")\n",
    "print(f\"  - Components used: {results['pca_analysis']['components_used']}\")\n",
    "print(f\"  - Variance explained: {results['pca_analysis']['variance_explained']:.2%}\")\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  - Accuracy:  {results['performance']['accuracy']:.2%}\")\n",
    "print(f\"  - F1 Score:  {results['performance']['f1_score']:.4f}\")\n",
    "print(f\"  - Precision: {results['performance']['precision']:.4f}\")\n",
    "print(f\"  - Recall:    {results['performance']['recall']:.4f}\")\n",
    "if results['performance']['ensemble_accuracy']:\n",
    "    print(f\"  - Ensemble Accuracy: {results['performance']['ensemble_accuracy']:.2%}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save results with error handling\n",
    "try:\n",
    "    if not IS_EMR:\n",
    "        os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "        results_file = os.path.join(RESULTS_PATH, f\"results_transfer_learning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "        \n",
    "        # Alternative method using custom JSON encoder\n",
    "        class NumpyEncoder(json.JSONEncoder):\n",
    "            def default(self, obj):\n",
    "                if isinstance(obj, np.integer):\n",
    "                    return int(obj)\n",
    "                elif isinstance(obj, np.floating):\n",
    "                    return float(obj)\n",
    "                elif isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                return super(NumpyEncoder, self).default(obj)\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2, cls=NumpyEncoder)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {results_file}\")\n",
    "    else:\n",
    "        # For EMR, you might want to save to S3\n",
    "        print(\"\\nRunning on EMR - implement S3 save if needed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nWarning: Could not save results file: {str(e)}\")\n",
    "    print(\"Results printed to console instead.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6abb80",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up broadcasts\n",
    "broadcast_weights.unpersist()\n",
    "broadcast_config.unpersist()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session closed.\")\n",
    "print(\"\\nExecution completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocp8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
