{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e5db9b",
   "metadata": {},
   "source": [
    "# Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afce15c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: Linux\n",
      "Is EMR: False\n",
      "Python version: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n",
      "Working on Linux platform.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import platform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Platform detection and configuration\n",
    "PLATFORM = platform.system()\n",
    "IS_EMR = 'EMR' in os.environ.get('AWS_EXECUTION_ENV', '')\n",
    "\n",
    "print(f\"Running on: {PLATFORM}\")\n",
    "print(f\"Is EMR: {IS_EMR}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Set environment variables based on platform\n",
    "if PLATFORM == \"Windows\":\n",
    "    # Windows configuration\n",
    "    print('Working on Windows platform.')\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "    if \"JAVA_HOME\" not in os.environ:\n",
    "        # Common Java paths on Windows\n",
    "        java_paths = [\"C:\\\\Program Files\\\\Java\\\\jdk-11\", \"C:\\\\Zulu\\\\zulu-11\"]\n",
    "        for path in java_paths:\n",
    "            if os.path.exists(path):\n",
    "                os.environ[\"JAVA_HOME\"] = path\n",
    "                break\n",
    "else:\n",
    "    # Linux configuration (Ubuntu/EMR)\n",
    "    print('Working on Linux platform.')\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "    if \"JAVA_HOME\" not in os.environ:\n",
    "        os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
    "        os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin:' + os.environ['PATH']\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "from typing import List, Tuple, Dict\n",
    "import json\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, udf, element_at, split, count, when, lit, coalesce\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType, BooleanType\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer, IndexToString\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c204aa",
   "metadata": {},
   "source": [
    "# Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2138bbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /mnt/data/Test1\n",
      "Results path: /mnt/data/Results\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    \"batch_size\": 32,\n",
    "    \"image_size\": (128, 128),  # Optimized for performance\n",
    "    \"num_features\": 512,  # Reduced for better performance\n",
    "    \"test_split\": 0.2,\n",
    "    \"seed\": 42,\n",
    "    # Random Forest parameters\n",
    "    \"num_trees\": 150,  # Reduced for faster training\n",
    "    \"max_depth\": 20,\n",
    "    \"min_instances_per_node\": 2,\n",
    "    \"feature_subset_strategy\": \"sqrt\"\n",
    "}\n",
    "\n",
    "# Path configuration\n",
    "if IS_EMR:\n",
    "    # EMR S3 paths\n",
    "    DATA_PATH = \"s3://your-bucket/fruit-images\"  # Update with your S3 bucket\n",
    "    RESULTS_PATH = \"s3://your-bucket/results\"\n",
    "else:\n",
    "    # Local paths\n",
    "    DATA_PATH = \"/mnt/data/Test1\"  # Update with your local path\n",
    "    RESULTS_PATH = \"/mnt/data/Results\"\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Results path: {RESULTS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992ea72",
   "metadata": {},
   "source": [
    "# Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b8f94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 11:16:42 WARN Utils: Your hostname, myserver resolves to a loopback address: 127.0.1.1; using 192.168.0.78 instead (on interface enp0s3)\n",
      "25/06/20 11:16:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/20 11:16:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.5\n",
      "Number of executors: 1\n",
      "Default parallelism: 8\n",
      "\n",
      "Spark initialization time: 1.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = time.time()\n",
    "initialization_start = time.time()\n",
    "\n",
    "# Create Spark session with optimized configuration\n",
    "def create_spark_session():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"FruitClassification_RandomForest_CrossPlatform\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", str(CONFIG[\"batch_size\"]))\n",
    "    \n",
    "    if not IS_EMR:\n",
    "        # Local configuration\n",
    "        builder = builder.master(\"local[*]\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "    else:\n",
    "        # EMR configuration\n",
    "        builder = builder \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\")\n",
    "    \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "spark = create_spark_session()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Print Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Number of executors: {sc._jsc.sc().getExecutorMemoryStatus().size()}\")\n",
    "print(f\"Default parallelism: {sc.defaultParallelism}\")\n",
    "\n",
    "initialization_time = time.time() - initialization_start\n",
    "print(f\"\\nSpark initialization time: {initialization_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d72da",
   "metadata": {},
   "source": [
    "# Load and Prepare Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d78e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: /mnt/data/Test1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total valid images loaded: 655\n",
      "\n",
      "Class distribution:\n",
      "  Apple Braeburn: 5 images\n",
      "  Apple Crimson Snow: 5 images\n",
      "  Apple Golden 1: 5 images\n",
      "  Apple Golden 2: 5 images\n",
      "  Apple Golden 3: 5 images\n",
      "  Apple Granny Smith: 5 images\n",
      "  Apple Pink Lady: 5 images\n",
      "  Apple Red 1: 5 images\n",
      "  Apple Red 2: 5 images\n",
      "  Apple Red 3: 5 images\n",
      "  Apple Red Delicious: 5 images\n",
      "  Apple Red Yellow 1: 5 images\n",
      "  Apple Red Yellow 2: 5 images\n",
      "  Apricot: 5 images\n",
      "  Avocado: 5 images\n",
      "  Avocado ripe: 5 images\n",
      "  Banana: 5 images\n",
      "  Banana Lady Finger: 5 images\n",
      "  Banana Red: 5 images\n",
      "  Beetroot: 5 images\n",
      "  Blueberry: 5 images\n",
      "  Cactus fruit: 5 images\n",
      "  Cantaloupe 1: 5 images\n",
      "  Cantaloupe 2: 5 images\n",
      "  Carambula: 5 images\n",
      "  Cauliflower: 5 images\n",
      "  Cherry 1: 5 images\n",
      "  Cherry 2: 5 images\n",
      "  Cherry Rainier: 5 images\n",
      "  Cherry Wax Black: 5 images\n",
      "  Cherry Wax Red: 5 images\n",
      "  Cherry Wax Yellow: 5 images\n",
      "  Chestnut: 5 images\n",
      "  Clementine: 5 images\n",
      "  Cocos: 5 images\n",
      "  Corn: 5 images\n",
      "  Corn Husk: 5 images\n",
      "  Cucumber Ripe: 5 images\n",
      "  Cucumber Ripe 2: 5 images\n",
      "  Dates: 5 images\n",
      "  Eggplant: 5 images\n",
      "  Fig: 5 images\n",
      "  Ginger Root: 5 images\n",
      "  Granadilla: 5 images\n",
      "  Grape Blue: 5 images\n",
      "  Grape Pink: 5 images\n",
      "  Grape White: 5 images\n",
      "  Grape White 2: 5 images\n",
      "  Grape White 3: 5 images\n",
      "  Grape White 4: 5 images\n",
      "  Grapefruit Pink: 5 images\n",
      "  Grapefruit White: 5 images\n",
      "  Guava: 5 images\n",
      "  Hazelnut: 5 images\n",
      "  Huckleberry: 5 images\n",
      "  Kaki: 5 images\n",
      "  Kiwi: 5 images\n",
      "  Kohlrabi: 5 images\n",
      "  Kumquats: 5 images\n",
      "  Lemon: 5 images\n",
      "  Lemon Meyer: 5 images\n",
      "  Limes: 5 images\n",
      "  Lychee: 5 images\n",
      "  Mandarine: 5 images\n",
      "  Mango: 5 images\n",
      "  Mango Red: 5 images\n",
      "  Mangostan: 5 images\n",
      "  Maracuja: 5 images\n",
      "  Melon Piel de Sapo: 5 images\n",
      "  Mulberry: 5 images\n",
      "  Nectarine: 5 images\n",
      "  Nectarine Flat: 5 images\n",
      "  Nut Forest: 5 images\n",
      "  Nut Pecan: 5 images\n",
      "  Onion Red: 5 images\n",
      "  Onion Red Peeled: 5 images\n",
      "  Onion White: 5 images\n",
      "  Orange: 5 images\n",
      "  Papaya: 5 images\n",
      "  Passion Fruit: 5 images\n",
      "  Peach: 5 images\n",
      "  Peach 2: 5 images\n",
      "  Peach Flat: 5 images\n",
      "  Pear: 5 images\n",
      "  Pear 2: 5 images\n",
      "  Pear Abate: 5 images\n",
      "  Pear Forelle: 5 images\n",
      "  Pear Kaiser: 5 images\n",
      "  Pear Monster: 5 images\n",
      "  Pear Red: 5 images\n",
      "  Pear Stone: 5 images\n",
      "  Pear Williams: 5 images\n",
      "  Pepino: 5 images\n",
      "  Pepper Green: 5 images\n",
      "  Pepper Orange: 5 images\n",
      "  Pepper Red: 5 images\n",
      "  Pepper Yellow: 5 images\n",
      "  Physalis: 5 images\n",
      "  Physalis with Husk: 5 images\n",
      "  Pineapple: 5 images\n",
      "  Pineapple Mini: 5 images\n",
      "  Pitahaya Red: 5 images\n",
      "  Plum: 5 images\n",
      "  Plum 2: 5 images\n",
      "  Plum 3: 5 images\n",
      "  Pomegranate: 5 images\n",
      "  Pomelo Sweetie: 5 images\n",
      "  Potato Red: 5 images\n",
      "  Potato Red Washed: 5 images\n",
      "  Potato Sweet: 5 images\n",
      "  Potato White: 5 images\n",
      "  Quince: 5 images\n",
      "  Rambutan: 5 images\n",
      "  Raspberry: 5 images\n",
      "  Redcurrant: 5 images\n",
      "  Salak: 5 images\n",
      "  Strawberry: 5 images\n",
      "  Strawberry Wedge: 5 images\n",
      "  Tamarillo: 5 images\n",
      "  Tangelo: 5 images\n",
      "  Tomato 1: 5 images\n",
      "  Tomato 2: 5 images\n",
      "  Tomato 3: 5 images\n",
      "  Tomato 4: 5 images\n",
      "  Tomato Cherry Red: 5 images\n",
      "  Tomato Heart: 5 images\n",
      "  Tomato Maroon: 5 images\n",
      "  Tomato Yellow: 5 images\n",
      "  Tomato not Ripened: 5 images\n",
      "  Walnut: 5 images\n",
      "  Watermelon: 5 images\n",
      "\n",
      "Data loading time: 4.30 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "loading_start = time.time()\n",
    "\n",
    "print(f\"Loading images from: {DATA_PATH}\")\n",
    "\n",
    "# Read images with labels from folder structure\n",
    "images_df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(DATA_PATH)\n",
    "\n",
    "# Extract labels from folder names\n",
    "images_df = images_df.withColumn('label', element_at(split(col('path'), '/'), -2))\n",
    "\n",
    "# Add image validation\n",
    "def validate_image(content):\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(content))\n",
    "        # Check if image can be processed\n",
    "        if img.size[0] > 0 and img.size[1] > 0:\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "validate_image_udf = udf(validate_image, BooleanType())\n",
    "images_df = images_df.withColumn('is_valid', validate_image_udf(col('content')))\n",
    "\n",
    "# Filter out corrupted images\n",
    "images_df = images_df.filter(col('is_valid') == True).drop('is_valid')\n",
    "\n",
    "# Cache the dataframe\n",
    "images_df.cache()\n",
    "\n",
    "# Count images per class\n",
    "image_count = images_df.count()\n",
    "class_distribution = images_df.groupBy('label').count().orderBy('label').collect()\n",
    "\n",
    "print(f\"\\nTotal valid images loaded: {image_count}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "for row in class_distribution:\n",
    "    print(f\"  {row['label']}: {row['count']} images\")\n",
    "\n",
    "loading_time = time.time() - loading_start\n",
    "print(f\"\\nData loading time: {loading_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412fc98",
   "metadata": {},
   "source": [
    "# Simple Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0760426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified feature extraction for better compatibility\n",
    "def extract_simple_features(image_bytes: bytes) -> List[float]:\n",
    "    \"\"\"Extract simple features from image\"\"\"\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(image_bytes))\n",
    "        img = img.resize(CONFIG[\"image_size\"])\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # 1. Color histogram features (RGB)\n",
    "        for channel in range(3):\n",
    "            hist, _ = np.histogram(img_array[:,:,channel], bins=16, range=(0, 256))\n",
    "            features.extend(hist.tolist())\n",
    "        \n",
    "        # 2. Statistical features per channel\n",
    "        for channel in range(3):\n",
    "            channel_data = img_array[:,:,channel].flatten()\n",
    "            features.extend([\n",
    "                float(np.mean(channel_data)),\n",
    "                float(np.std(channel_data)),\n",
    "                float(np.min(channel_data)),\n",
    "                float(np.max(channel_data))\n",
    "            ])\n",
    "        \n",
    "        # 3. Simple texture features\n",
    "        gray = np.dot(img_array[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "        \n",
    "        # Gradient features\n",
    "        dx = np.diff(gray, axis=1)\n",
    "        dy = np.diff(gray, axis=0)\n",
    "        \n",
    "        features.extend([\n",
    "            float(np.mean(np.abs(dx))),\n",
    "            float(np.std(dx)),\n",
    "            float(np.mean(np.abs(dy))),\n",
    "            float(np.std(dy))\n",
    "        ])\n",
    "        \n",
    "        # Ensure we have exactly num_features\n",
    "        if len(features) < CONFIG[\"num_features\"]:\n",
    "            features.extend([0.0] * (CONFIG[\"num_features\"] - len(features)))\n",
    "        else:\n",
    "            features = features[:CONFIG[\"num_features\"]]\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return zeros if error\n",
    "        return [0.0] * CONFIG[\"num_features\"]\n",
    "\n",
    "# Create UDF for feature extraction\n",
    "extract_features_udf = udf(extract_simple_features, ArrayType(FloatType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b01a1",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c600385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:============================>                            (8 + 8) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 655 images\n",
      "\n",
      "Feature extraction time: 1.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Extract features and create vectors directly\n",
    "feature_extraction_start = time.time()\n",
    "\n",
    "print(\"Extracting features from images...\")\n",
    "\n",
    "# Repartition for optimal performance\n",
    "optimal_partitions = min(sc.defaultParallelism * 2, 100)\n",
    "images_df_repartitioned = images_df.repartition(optimal_partitions)\n",
    "\n",
    "# Apply feature extraction\n",
    "features_df = images_df_repartitioned.select(\n",
    "    col(\"path\"),\n",
    "    col(\"label\"),\n",
    "    extract_features_udf(col(\"content\")).alias(\"features\")\n",
    ")\n",
    "\n",
    "# Convert array directly to vector\n",
    "def array_to_vector(features):\n",
    "    if features is None:\n",
    "        return Vectors.dense([0.0] * CONFIG[\"num_features\"])\n",
    "    return Vectors.dense(features)\n",
    "\n",
    "array_to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "\n",
    "# Create features vector directly\n",
    "features_vector_df = features_df.select(\n",
    "    col(\"path\"),\n",
    "    col(\"label\"),\n",
    "    array_to_vector_udf(col(\"features\")).alias(\"features_vector\")\n",
    ")\n",
    "\n",
    "# Cache processed features\n",
    "features_vector_df.cache()\n",
    "processed_count = features_vector_df.count()\n",
    "\n",
    "print(f\"Successfully processed {processed_count} images\")\n",
    "\n",
    "feature_extraction_time = time.time() - feature_extraction_start\n",
    "print(f\"\\nFeature extraction time: {feature_extraction_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a07258",
   "metadata": {},
   "source": [
    "# Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5ffb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['Apple Braeburn', 'Apple Crimson Snow', 'Apple Golden 1', 'Apple Golden 2', 'Apple Golden 3', 'Apple Granny Smith', 'Apple Pink Lady', 'Apple Red 1', 'Apple Red 2', 'Apple Red 3', 'Apple Red Delicious', 'Apple Red Yellow 1', 'Apple Red Yellow 2', 'Apricot', 'Avocado', 'Avocado ripe', 'Banana', 'Banana Lady Finger', 'Banana Red', 'Beetroot', 'Blueberry', 'Cactus fruit', 'Cantaloupe 1', 'Cantaloupe 2', 'Carambula', 'Cauliflower', 'Cherry 1', 'Cherry 2', 'Cherry Rainier', 'Cherry Wax Black', 'Cherry Wax Red', 'Cherry Wax Yellow', 'Chestnut', 'Clementine', 'Cocos', 'Corn', 'Corn Husk', 'Cucumber Ripe', 'Cucumber Ripe 2', 'Dates', 'Eggplant', 'Fig', 'Ginger Root', 'Granadilla', 'Grape Blue', 'Grape Pink', 'Grape White', 'Grape White 2', 'Grape White 3', 'Grape White 4', 'Grapefruit Pink', 'Grapefruit White', 'Guava', 'Hazelnut', 'Huckleberry', 'Kaki', 'Kiwi', 'Kohlrabi', 'Kumquats', 'Lemon', 'Lemon Meyer', 'Limes', 'Lychee', 'Mandarine', 'Mango', 'Mango Red', 'Mangostan', 'Maracuja', 'Melon Piel de Sapo', 'Mulberry', 'Nectarine', 'Nectarine Flat', 'Nut Forest', 'Nut Pecan', 'Onion Red', 'Onion Red Peeled', 'Onion White', 'Orange', 'Papaya', 'Passion Fruit', 'Peach', 'Peach 2', 'Peach Flat', 'Pear', 'Pear 2', 'Pear Abate', 'Pear Forelle', 'Pear Kaiser', 'Pear Monster', 'Pear Red', 'Pear Stone', 'Pear Williams', 'Pepino', 'Pepper Green', 'Pepper Orange', 'Pepper Red', 'Pepper Yellow', 'Physalis', 'Physalis with Husk', 'Pineapple', 'Pineapple Mini', 'Pitahaya Red', 'Plum', 'Plum 2', 'Plum 3', 'Pomegranate', 'Pomelo Sweetie', 'Potato Red', 'Potato Red Washed', 'Potato Sweet', 'Potato White', 'Quince', 'Rambutan', 'Raspberry', 'Redcurrant', 'Salak', 'Strawberry', 'Strawberry Wedge', 'Tamarillo', 'Tangelo', 'Tomato 1', 'Tomato 2', 'Tomato 3', 'Tomato 4', 'Tomato Cherry Red', 'Tomato Heart', 'Tomato Maroon', 'Tomato Yellow', 'Tomato not Ripened', 'Walnut', 'Watermelon']\n",
      "Number of classes: 131\n",
      "\n",
      "Training samples: 526\n",
      "Test samples: 129\n",
      "\n",
      "Data preparation time: 1.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for ML\n",
    "preparation_start = time.time()\n",
    "\n",
    "# Index labels\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "label_indexer_model = label_indexer.fit(features_vector_df)\n",
    "\n",
    "# Get label mappings\n",
    "labels = label_indexer_model.labels\n",
    "print(f\"Classes found: {labels}\")\n",
    "print(f\"Number of classes: {len(labels)}\")\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = features_vector_df.randomSplit([1-CONFIG[\"test_split\"], CONFIG[\"test_split\"]], \n",
    "                                                   seed=CONFIG[\"seed\"])\n",
    "\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "\n",
    "print(f\"\\nTraining samples: {train_count}\")\n",
    "print(f\"Test samples: {test_count}\")\n",
    "\n",
    "preparation_time = time.time() - preparation_start\n",
    "print(f\"\\nData preparation time: {preparation_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f60790",
   "metadata": {},
   "source": [
    "# Train Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "852d0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest model...\n",
      "Training with 150 trees and max depth 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 11:16:52 WARN DAGScheduler: Broadcasting large task binary with size 1587.8 KiB\n",
      "25/06/20 11:16:53 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/06/20 11:16:54 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/06/20 11:16:55 WARN DAGScheduler: Broadcasting large task binary with size 1122.6 KiB\n",
      "25/06/20 11:16:55 WARN DAGScheduler: Broadcasting large task binary with size 8.4 MiB\n",
      "25/06/20 11:16:56 WARN DAGScheduler: Broadcasting large task binary with size 1386.9 KiB\n",
      "25/06/20 11:16:57 WARN DAGScheduler: Broadcasting large task binary with size 11.9 MiB\n",
      "25/06/20 11:16:57 WARN DAGScheduler: Broadcasting large task binary with size 1593.0 KiB\n",
      "25/06/20 11:16:58 WARN DAGScheduler: Broadcasting large task binary with size 15.7 MiB\n",
      "25/06/20 11:16:59 WARN DAGScheduler: Broadcasting large task binary with size 1730.1 KiB\n",
      "25/06/20 11:17:00 WARN DAGScheduler: Broadcasting large task binary with size 19.9 MiB\n",
      "25/06/20 11:17:01 WARN DAGScheduler: Broadcasting large task binary with size 1814.6 KiB\n",
      "25/06/20 11:17:02 WARN DAGScheduler: Broadcasting large task binary with size 24.0 MiB\n",
      "25/06/20 11:17:03 WARN DAGScheduler: Broadcasting large task binary with size 1844.2 KiB\n",
      "25/06/20 11:17:04 WARN DAGScheduler: Broadcasting large task binary with size 28.0 MiB\n",
      "25/06/20 11:17:05 WARN DAGScheduler: Broadcasting large task binary with size 1704.9 KiB\n",
      "25/06/20 11:17:06 WARN DAGScheduler: Broadcasting large task binary with size 31.8 MiB\n",
      "25/06/20 11:17:07 WARN DAGScheduler: Broadcasting large task binary with size 1553.6 KiB\n",
      "25/06/20 11:17:08 WARN DAGScheduler: Broadcasting large task binary with size 35.2 MiB\n",
      "25/06/20 11:17:09 WARN DAGScheduler: Broadcasting large task binary with size 1398.9 KiB\n",
      "25/06/20 11:17:09 WARN DAGScheduler: Broadcasting large task binary with size 37.3 MiB\n",
      "25/06/20 11:17:10 WARN DAGScheduler: Broadcasting large task binary with size 1157.7 KiB\n",
      "25/06/20 11:17:11 WARN DAGScheduler: Broadcasting large task binary with size 38.2 MiB\n",
      "25/06/20 11:17:12 WARN DAGScheduler: Broadcasting large task binary with size 37.2 MiB\n",
      "25/06/20 11:17:13 WARN DAGScheduler: Broadcasting large task binary with size 35.9 MiB\n",
      "25/06/20 11:17:14 WARN DAGScheduler: Broadcasting large task binary with size 29.9 MiB\n",
      "25/06/20 11:17:14 WARN DAGScheduler: Broadcasting large task binary with size 25.1 MiB\n",
      "25/06/20 11:17:14 WARN DAGScheduler: Broadcasting large task binary with size 20.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training time: 25.64 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "training_start = time.time()\n",
    "\n",
    "print(\"Training Random Forest model...\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vector\", \n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Create Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label_index\",\n",
    "    numTrees=CONFIG[\"num_trees\"],\n",
    "    maxDepth=CONFIG[\"max_depth\"],\n",
    "    minInstancesPerNode=CONFIG[\"min_instances_per_node\"],\n",
    "    featureSubsetStrategy=CONFIG[\"feature_subset_strategy\"],\n",
    "    seed=CONFIG[\"seed\"]\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer_model, scaler, rf])\n",
    "\n",
    "# Train model\n",
    "print(f\"Training with {CONFIG['num_trees']} trees and max depth {CONFIG['max_depth']}...\")\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Get the Random Forest model from the pipeline\n",
    "rf_model = model.stages[-1]\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "print(f\"\\nModel training time: {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ec372",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af3c13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 11:17:15 WARN DAGScheduler: Broadcasting large task binary with size 27.3 MiB\n",
      "25/06/20 11:17:16 WARN DAGScheduler: Broadcasting large task binary with size 27.3 MiB\n",
      "25/06/20 11:17:16 WARN DAGScheduler: Broadcasting large task binary with size 27.3 MiB\n",
      "25/06/20 11:17:16 WARN DAGScheduler: Broadcasting large task binary with size 27.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model evaluation time: 1.57 seconds\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.7519 (75.19%)\n",
      "F1 Score:  0.7602\n",
      "Precision: 0.8101\n",
      "Recall:    0.7519\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "evaluation_start = time.time()\n",
    "\n",
    "print(\"Evaluating model performance...\")\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Calculate metrics\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = rf_model.featureImportances\n",
    "\n",
    "evaluation_time = time.time() - evaluation_start\n",
    "print(f\"\\nModel evaluation time: {evaluation_time:.2f} seconds\")\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODEL PERFORMANCE RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"F1 Score:  {f1_score:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f2617",
   "metadata": {},
   "source": [
    "# Save Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe624a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXECUTION SUMMARY\n",
      "============================================================\n",
      "Platform: Linux (Local)\n",
      "Spark Version: 3.5.5\n",
      "Executors: 1\n",
      "Parallelism: 8\n",
      "\n",
      "Data Summary:\n",
      "  - Total images: 655\n",
      "  - Successfully processed: 655\n",
      "  - Number of classes: 131\n",
      "\n",
      "Model Configuration:\n",
      "  - Type: RandomForestClassifier\n",
      "  - Trees: 150\n",
      "  - Max Depth: 20\n",
      "\n",
      "Model Performance:\n",
      "  - Accuracy:  75.19%\n",
      "  - F1 Score:  0.7602\n",
      "  - Precision: 0.8101\n",
      "  - Recall:    0.7519\n",
      "\n",
      "Execution Times:\n",
      "  - Initialization: 1.86s\n",
      "  - Data Loading: 4.30s\n",
      "  - Feature Extraction: 1.23s\n",
      "  - Data Preparation: 1.02s\n",
      "  - Model Training: 25.64s\n",
      "  - Model Evaluation: 1.57s\n",
      "  - TOTAL TIME: 35.69s\n",
      "============================================================\n",
      "\n",
      "Results saved to: /mnt/data/Results/results_20250620_111717.json\n"
     ]
    }
   ],
   "source": [
    "# Calculate total time\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Prepare results\n",
    "results = {\n",
    "    \"platform\": {\n",
    "        \"system\": PLATFORM,\n",
    "        \"is_emr\": IS_EMR,\n",
    "        \"spark_version\": spark.version,\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"executors\": sc._jsc.sc().getExecutorMemoryStatus().size(),\n",
    "        \"default_parallelism\": sc.defaultParallelism\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"total_images\": image_count,\n",
    "        \"processed_images\": processed_count,\n",
    "        \"train_samples\": train_count,\n",
    "        \"test_samples\": test_count,\n",
    "        \"num_classes\": len(labels),\n",
    "        \"classes\": labels\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"RandomForestClassifier\",\n",
    "        \"num_trees\": CONFIG[\"num_trees\"],\n",
    "        \"max_depth\": CONFIG[\"max_depth\"],\n",
    "        \"num_features\": CONFIG[\"num_features\"]\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"f1_score\": float(f1_score),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall)\n",
    "    },\n",
    "    \"timing\": {\n",
    "        \"initialization_time\": initialization_time,\n",
    "        \"loading_time\": loading_time,\n",
    "        \"feature_extraction_time\": feature_extraction_time,\n",
    "        \"preparation_time\": preparation_time,\n",
    "        \"training_time\": training_time,\n",
    "        \"evaluation_time\": evaluation_time,\n",
    "        \"total_time\": total_time\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Platform: {results['platform']['system']} {'(EMR)' if IS_EMR else '(Local)'}\")\n",
    "print(f\"Spark Version: {results['platform']['spark_version']}\")\n",
    "print(f\"Executors: {results['platform']['executors']}\")\n",
    "print(f\"Parallelism: {results['platform']['default_parallelism']}\")\n",
    "print(f\"\\nData Summary:\")\n",
    "print(f\"  - Total images: {results['data']['total_images']}\")\n",
    "print(f\"  - Successfully processed: {results['data']['processed_images']}\")\n",
    "print(f\"  - Number of classes: {results['data']['num_classes']}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Type: {results['model']['type']}\")\n",
    "print(f\"  - Trees: {results['model']['num_trees']}\")\n",
    "print(f\"  - Max Depth: {results['model']['max_depth']}\")\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  - Accuracy:  {results['performance']['accuracy']:.2%}\")\n",
    "print(f\"  - F1 Score:  {results['performance']['f1_score']:.4f}\")\n",
    "print(f\"  - Precision: {results['performance']['precision']:.4f}\")\n",
    "print(f\"  - Recall:    {results['performance']['recall']:.4f}\")\n",
    "print(f\"\\nExecution Times:\")\n",
    "print(f\"  - Initialization: {results['timing']['initialization_time']:.2f}s\")\n",
    "print(f\"  - Data Loading: {results['timing']['loading_time']:.2f}s\")\n",
    "print(f\"  - Feature Extraction: {results['timing']['feature_extraction_time']:.2f}s\")\n",
    "print(f\"  - Data Preparation: {results['timing']['preparation_time']:.2f}s\")\n",
    "print(f\"  - Model Training: {results['timing']['training_time']:.2f}s\")\n",
    "print(f\"  - Model Evaluation: {results['timing']['evaluation_time']:.2f}s\")\n",
    "print(f\"  - TOTAL TIME: {results['timing']['total_time']:.2f}s\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save results\n",
    "if not IS_EMR:\n",
    "    # Save locally\n",
    "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "    results_file = os.path.join(RESULTS_PATH, f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nResults saved to: {results_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c25531",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04eb5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session closed.\n",
      "\n",
      "Execution completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session closed.\")\n",
    "print(\"\\nExecution completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
